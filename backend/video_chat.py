"""
ðŸ“¹ Video Chat - Ú†Øª ÙˆÛŒØ¯ÛŒÙˆÛŒÛŒ Real-time Ø¨Ø§ Gemini
ØªØ¹Ø§Ù…Ù„ ÙˆÛŒØ¯ÛŒÙˆÛŒÛŒ Ù…Ø¯Ø§ÙˆÙ… Ø¨Ø§ AI Ú©Ù‡ Ú†Ù‡Ø±Ù‡ Ùˆ Ø§Ø­Ø³Ø§Ø³ Ú©Ø§Ø±Ø¨Ø± Ø±Ùˆ Ù…ÛŒâ€ŒØ¨ÛŒÙ†Ù‡

ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§:
- Ø§Ø³ØªØ±ÛŒÙ… ÙˆÛŒØ¯ÛŒÙˆ Ø§Ø² Ø¯ÙˆØ±Ø¨ÛŒÙ† Ú©Ø§Ø±Ø¨Ø±
- ØªØ­Ù„ÛŒÙ„ real-time Ú†Ù‡Ø±Ù‡ Ùˆ Ø§Ø­Ø³Ø§Ø³
- Ù¾Ø§Ø³Ø® ØµÙˆØªÛŒ/Ù…ØªÙ†ÛŒ Ù…ØªÙ†Ø§Ø³Ø¨ Ø¨Ø§ Ø­Ø§Ù„ Ú©Ø§Ø±Ø¨Ø±
- Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ ØºØ°Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ø­Ø³Ø§Ø³
- ðŸ§  ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø§Ø² Ù…Ú©Ø§Ù„Ù…Ø§Øª ÙˆÛŒØ¯ÛŒÙˆÛŒÛŒ
- ðŸŽ¤ Ú†Øª ØµÙˆØªÛŒ Ù…Ø¯Ø§ÙˆÙ…
"""

import asyncio
import base64
import json
from datetime import datetime
from typing import Optional, Dict, AsyncGenerator, List
import requests
from config import GAPGPT_API_KEY
import smart_learner  # ðŸ§  ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯

# Ù…Ø¯Ù„ Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ ÙˆÛŒØ¯ÛŒÙˆ (Ø¢Ù¾Ø¯ÛŒØª Ø´Ø¯Ù‡)
VIDEO_MODEL = "gemini-live-2.5-flash-preview"  # ðŸ†• Ø¨Ù‡ØªØ± Ø¨Ø±Ø§ÛŒ real-time
VISION_MODEL = "gemini-3-pro-image-preview"    # ðŸ†• ØªØ­Ù„ÛŒÙ„ ØªØµÙˆÛŒØ± Ø¯Ù‚ÛŒÙ‚â€ŒØªØ±

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø§Ø­Ø³Ø§Ø³
EMOTION_RESPONSES = {
    "happy": {
        "fa": "Ø®ÙˆØ´Ø­Ø§Ù„",
        "response_tone": "Ù¾Ø±Ø§Ù†Ø±Ú˜ÛŒ Ùˆ Ø´Ø§Ø¯",
        "food_suggestion": "ÛŒÙ‡ ØºØ°Ø§ÛŒ Ø¬Ø´Ù†ÛŒ Ù…Ø«Ù„ Ù¾ÛŒØªØ²Ø§ ÛŒØ§ Ø¨Ø±Ú¯Ø± Ú†Ø·ÙˆØ±Ù‡ØŸ ðŸŽ‰"
    },
    "sad": {
        "fa": "ØºÙ…Ú¯ÛŒÙ†",
        "response_tone": "Ù‡Ù…Ø¯Ø±Ø¯Ø§Ù†Ù‡ Ùˆ Ù…Ù‡Ø±Ø¨Ø§Ù†",
        "food_suggestion": "ÛŒÙ‡ Ø³ÙˆÙ¾ Ú¯Ø±Ù… ÛŒØ§ Ø´Ú©Ù„Ø§Øª Ø¯Ø§Øº Ø­Ø§Ù„ØªÙˆ Ø®ÙˆØ¨ Ù…ÛŒÚ©Ù†Ù‡ ðŸ¤—"
    },
    "angry": {
        "fa": "Ø¹ØµØ¨Ø§Ù†ÛŒ", 
        "response_tone": "Ø¢Ø±Ø§Ù… Ùˆ ØµØ¨ÙˆØ±Ø§Ù†Ù‡",
        "food_suggestion": "ÛŒÙ‡ Ù†ÙˆØ´ÛŒØ¯Ù†ÛŒ Ø®Ù†Ú© Ø¨Ø²Ù† Ø¢Ø±ÙˆÙ… Ø´ÛŒ ðŸ§Š"
    },
    "surprised": {
        "fa": "Ù…ØªØ¹Ø¬Ø¨",
        "response_tone": "Ù‡ÛŒØ¬Ø§Ù†â€ŒØ²Ø¯Ù‡",
        "food_suggestion": "Ø§Ù…Ø±ÙˆØ² ÛŒÙ‡ Ú†ÛŒØ² Ø¬Ø¯ÛŒØ¯ Ø§Ù…ØªØ­Ø§Ù† Ú©Ù†! ðŸŒŸ"
    },
    "fearful": {
        "fa": "Ù†Ú¯Ø±Ø§Ù†",
        "response_tone": "Ø§Ø·Ù…ÛŒÙ†Ø§Ù†â€ŒØ¨Ø®Ø´",
        "food_suggestion": "ÛŒÙ‡ Ú†Ø§ÛŒ Ú¯Ø±Ù… Ø¨Ø§ Ú©ÛŒÚ© Ø¢Ø±ÙˆÙ…Øª Ù…ÛŒÚ©Ù†Ù‡ â˜•"
    },
    "disgusted": {
        "fa": "Ù†Ø§Ø±Ø§Ø­Øª",
        "response_tone": "Ø¯Ø±Ú©â€ŒÚ©Ù†Ù†Ø¯Ù‡",
        "food_suggestion": "ÛŒÙ‡ ØºØ°Ø§ÛŒ Ø³Ø§Ø¯Ù‡ Ùˆ Ø³Ø¨Ú© Ú†Ø·ÙˆØ±Ù‡ØŸ"
    },
    "neutral": {
        "fa": "Ø¹Ø§Ø¯ÛŒ",
        "response_tone": "Ø¯ÙˆØ³ØªØ§Ù†Ù‡",
        "food_suggestion": "Ú†ÛŒ Ù…ÛŒÙ„ Ø¯Ø§Ø±ÛŒ Ø§Ù…Ø±ÙˆØ²ØŸ ðŸ˜Š"
    }
}

# System Prompt Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ ÙˆÛŒØ¯ÛŒÙˆ
VIDEO_ANALYSIS_PROMPT = """ØªÙˆ Ø±ÙˆÚ˜Ø§Ù† Ù‡Ø³ØªÛŒØŒ Ø¯Ø³ØªÛŒØ§Ø± Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø±Ø³ØªÙˆØ±Ø§Ù† Ú©Ù‡ Ø¯Ø§Ø±Ù‡ ÙˆÛŒØ¯ÛŒÙˆÛŒ Ú©Ø§Ø±Ø¨Ø± Ø±Ùˆ Ù…ÛŒâ€ŒØ¨ÛŒÙ†Ù‡.

ÙˆØ¸ÛŒÙÙ‡â€ŒØ§Øª:
1. Ø§Ø­Ø³Ø§Ø³ Ú©Ø§Ø±Ø¨Ø± Ø±Ùˆ Ø§Ø² Ú†Ù‡Ø±Ù‡â€ŒØ§Ø´ ØªØ´Ø®ÛŒØµ Ø¨Ø¯Ù‡ (happy, sad, angry, surprised, fearful, disgusted, neutral)
2. Ø³Ù† ØªÙ‚Ø±ÛŒØ¨ÛŒ Ùˆ Ø¬Ù†Ø³ÛŒØª Ø±Ùˆ Ø­Ø¯Ø³ Ø¨Ø²Ù†
3. Ù…Ø­ÛŒØ· Ø±Ùˆ ØªÙˆØµÛŒÙ Ú©Ù† (Ø®ÙˆÙ†Ù‡ØŒ Ø¯ÙØªØ±ØŒ Ø¨ÛŒØ±ÙˆÙ†)
4. Ø§Ú¯Ù‡ ØºØ°Ø§ÛŒÛŒ Ø¯Ø± ØªØµÙˆÛŒØ± Ù‡Ø³ØªØŒ ØªÙˆØ¶ÛŒØ­ Ø¨Ø¯Ù‡

Ø®Ø±ÙˆØ¬ÛŒ JSON:
{
    "emotion": "happy/sad/angry/...",
    "emotion_confidence": 0.0-1.0,
    "age_range": "20-30",
    "gender": "male/female",
    "environment": "home/office/outdoor/restaurant",
    "food_visible": true/false,
    "food_description": "ØªÙˆØ¶ÛŒØ­ ØºØ°Ø§ Ø§Ú¯Ù‡ Ù‡Ø³Øª",
    "face_count": 1,
    "suggestion": "Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ ØºØ°Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ø­Ø³Ø§Ø³"
}
"""

CHAT_WITH_VIDEO_PROMPT = """ØªÙˆ Ø±ÙˆÚ˜Ø§Ù† Ù‡Ø³ØªÛŒØŒ Ø¯Ø³ØªÛŒØ§Ø± Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø±Ø³ØªÙˆØ±Ø§Ù†.
Ø§Ù„Ø§Ù† Ø¯Ø§Ø±ÛŒ Ø¨Ø§ Ú©Ø§Ø±Ø¨Ø± ÙˆÛŒØ¯ÛŒÙˆ Ú†Øª Ù…ÛŒâ€ŒÚ©Ù†ÛŒ Ùˆ Ú†Ù‡Ø±Ù‡â€ŒØ§Ø´ Ø±Ùˆ Ù…ÛŒâ€ŒØ¨ÛŒÙ†ÛŒ.

Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø§Ø² ØªØµÙˆÛŒØ±:
- Ø§Ø­Ø³Ø§Ø³ Ú©Ø§Ø±Ø¨Ø±: {emotion} ({emotion_fa})
- Ù…Ø­ÛŒØ·: {environment}

Ù‚ÙˆØ§Ù†ÛŒÙ†:
1. Ù…ØªÙ†Ø§Ø³Ø¨ Ø¨Ø§ Ø§Ø­Ø³Ø§Ø³ Ú©Ø§Ø±Ø¨Ø± Ø¬ÙˆØ§Ø¨ Ø¨Ø¯Ù‡
2. ÙØ§Ø±Ø³ÛŒ Ù…Ø­Ø§ÙˆØ±Ù‡â€ŒØ§ÛŒ ØµØ­Ø¨Øª Ú©Ù†
3. Ú©ÙˆØªØ§Ù‡ Ùˆ Ù…ÙÛŒØ¯ Ø¨Ø§Ø´ (Ø¨Ø±Ø§ÛŒ Ù…Ú©Ø§Ù„Ù…Ù‡ ÙˆÛŒØ¯ÛŒÙˆÛŒÛŒ)
4. Ø§Ú¯Ù‡ ØºÙ…Ú¯ÛŒÙ† ÛŒØ§ Ø¹ØµØ¨Ø§Ù†ÛŒÙ‡ØŒ Ù‡Ù…Ø¯Ø±Ø¯ÛŒ Ú©Ù†

Ù¾ÛŒØ§Ù… Ú©Ø§Ø±Ø¨Ø±: {message}
"""


class VideoChatSession:
    """
    ÛŒÚ© session Ú†Øª ÙˆÛŒØ¯ÛŒÙˆÛŒÛŒ Ù…Ø¯Ø§ÙˆÙ… Ø¨Ø§ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ
    
    Ù‚Ø§Ø¨Ù„ÛŒØªâ€ŒÙ‡Ø§:
    - Ù…Ú©Ø§Ù„Ù…Ù‡ Ù…Ø¯Ø§ÙˆÙ… Ø¨Ø§ Ø­ÙØ¸ context
    - ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø§Ø² Ú†Øª
    - ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³ real-time
    - ØªØ´Ø®ÛŒØµ ØªØºÛŒÛŒØ± Ø§Ø­Ø³Ø§Ø³
    """
    
    def __init__(self, user_id: int = None):
        self.user_id = user_id
        self.session_id = None
        self.is_active = False
        self.current_emotion = "neutral"
        self.emotion_history: List[Dict] = []
        self.frame_count = 0
        self.last_analysis = None
        self.api_url = "https://api.gapgpt.app/v1beta/models"
        
        # ðŸ§  Ù…Ú©Ø§Ù„Ù…Ù‡ Ù…Ø¯Ø§ÙˆÙ…
        self.conversation_history: List[Dict] = []
        self.max_history = 20  # Ø­Ø¯Ø§Ú©Ø«Ø± ØªØ§Ø±ÛŒØ®Ú†Ù‡
        self.learned_info: List[str] = []  # Ø§Ø·Ù„Ø§Ø¹Ø§Øª ÛŒØ§Ø¯ Ú¯Ø±ÙØªÙ‡ Ø´Ø¯Ù‡
        
        # ðŸ“Š Ø¢Ù…Ø§Ø± session
        self.start_time = None
        self.message_count = 0
        self.emotion_changes = 0
        self.last_emotion_change = None
        
    async def start(self):
        """Ø´Ø±ÙˆØ¹ session"""
        self.is_active = True
        self.start_time = datetime.now()
        self.session_id = f"video_{self.user_id or 'guest'}_{self.start_time.strftime('%Y%m%d%H%M%S')}"
        
        # Ù¾ÛŒØ§Ù… Ø®ÙˆØ´â€ŒØ¢Ù…Ø¯Ú¯ÙˆÛŒÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù¾Ø±ÙˆÙØ§ÛŒÙ„ Ú©Ø§Ø±Ø¨Ø±
        welcome = "Ø³Ù„Ø§Ù…! ðŸ‘‹ Ù…Ù† Ø±ÙˆÚ˜Ø§Ù† Ù‡Ø³ØªÙ… Ùˆ Ø¯Ø§Ø±Ù… Ù…ÛŒâ€ŒØ¨ÛŒÙ†Ù…Øª!"
        
        if self.user_id:
            try:
                profile_summary = smart_learner.get_profile_summary(self.user_id)
                if profile_summary:
                    welcome += f"\n{profile_summary[:100]}..."
            except:
                pass
        
        self.conversation_history.append({
            "role": "assistant",
            "content": welcome,
            "timestamp": datetime.now().isoformat()
        })
        
        print(f"ðŸ“¹ Video chat started: {self.session_id}")
        return {
            "session_id": self.session_id, 
            "status": "started",
            "welcome": welcome
        }
    
    async def stop(self):
        """Ù¾Ø§ÛŒØ§Ù† session Ø¨Ø§ Ø®Ù„Ø§ØµÙ‡ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ"""
        self.is_active = False
        duration = (datetime.now() - self.start_time).seconds if self.start_time else 0
        
        print(f"ðŸ“¹ Video chat stopped: {self.session_id}")
        
        # ðŸ§  ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù†Ù‡Ø§ÛŒÛŒ Ø§Ø² Ú©Ù„ Ù…Ú©Ø§Ù„Ù…Ù‡
        if self.user_id and self.conversation_history:
            try:
                # ØªØ±Ú©ÛŒØ¨ Ù‡Ù…Ù‡ Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ÛŒ Ú©Ø§Ø±Ø¨Ø±
                user_messages = " ".join([
                    msg["content"] for msg in self.conversation_history 
                    if msg["role"] == "user"
                ])
                if user_messages:
                    learn_result = smart_learner.learn_from_chat(self.user_id, user_messages)
                    if learn_result.get("learned"):
                        self.learned_info.extend(learn_result.get("categories", []))
                        print(f"  ðŸ§  Final learning: {learn_result.get('categories', [])}")
            except Exception as e:
                print(f"  âš ï¸ Final learning error: {e}")
        
        # Ø®Ù„Ø§ØµÙ‡ Ø§Ø­Ø³Ø§Ø³Ø§Øª
        emotion_summary = {}
        for e in self.emotion_history:
            emotion = e.get("emotion", "neutral")
            emotion_summary[emotion] = emotion_summary.get(emotion, 0) + 1
        
        dominant = max(emotion_summary, key=emotion_summary.get) if emotion_summary else "neutral"
        
        return {
            "session_id": self.session_id,
            "status": "stopped",
            "duration_seconds": duration,
            "message_count": self.message_count,
            "frames_analyzed": self.frame_count,
            "dominant_emotion": dominant,
            "emotion_changes": self.emotion_changes,
            "emotion_summary": emotion_summary,
            "learned_categories": list(set(self.learned_info))
        }
    
    async def analyze_frame(self, frame_base64: str) -> Dict:
        """
        ØªØ­Ù„ÛŒÙ„ ÛŒÚ© ÙØ±ÛŒÙ… Ø§Ø² ÙˆÛŒØ¯ÛŒÙˆ
        
        Args:
            frame_base64: ØªØµÙˆÛŒØ± Ø¨Ù‡ ØµÙˆØ±Øª base64
            
        Returns:
            dict Ø¨Ø§ Ø§Ø·Ù„Ø§Ø¹Ø§Øª ØªØ­Ù„ÛŒÙ„
        """
        if not self.is_active:
            return {"error": "Session not active"}
        
        self.frame_count += 1
        
        try:
            # Ø­Ø°Ù prefix Ø§Ú¯Ù‡ Ø¯Ø§Ø±Ù‡
            if "," in frame_base64:
                frame_base64 = frame_base64.split(",")[1]
            
            # Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø¨Ù‡ Gemini Vision
            url = f"{self.api_url}/{VISION_MODEL}:generateContent"
            headers = {
                "Authorization": f"Bearer {GAPGPT_API_KEY}",
                "Content-Type": "application/json"
            }
            
            data = {
                "contents": [{
                    "parts": [
                        {"text": VIDEO_ANALYSIS_PROMPT},
                        {
                            "inline_data": {
                                "mime_type": "image/jpeg",
                                "data": frame_base64
                            }
                        }
                    ]
                }],
                "generationConfig": {
                    "temperature": 0.1,
                    "maxOutputTokens": 500
                }
            }
            
            response = requests.post(url, headers=headers, json=data, timeout=10)
            
            if response.ok:
                result = response.json()
                text = result['candidates'][0]['content']['parts'][0]['text'].strip()
                
                # Ù¾Ø§Ø±Ø³ JSON Ø§Ø² Ù¾Ø§Ø³Ø®
                try:
                    # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† JSON Ø¯Ø± Ù…ØªÙ†
                    import re
                    json_match = re.search(r'\{.*\}', text, re.DOTALL)
                    if json_match:
                        analysis = json.loads(json_match.group())
                    else:
                        analysis = {"emotion": "neutral", "error": "No JSON found"}
                except json.JSONDecodeError:
                    analysis = {"emotion": "neutral", "raw_response": text}
                
                # Ø°Ø®ÛŒØ±Ù‡ Ø§Ø­Ø³Ø§Ø³
                emotion = analysis.get("emotion", "neutral")
                prev_emotion = self.current_emotion
                
                # ØªØ´Ø®ÛŒØµ ØªØºÛŒÛŒØ± Ø§Ø­Ø³Ø§Ø³
                emotion_changed = prev_emotion != emotion
                if emotion_changed:
                    self.emotion_changes += 1
                    self.last_emotion_change = {
                        "from": prev_emotion,
                        "to": emotion,
                        "timestamp": datetime.now().isoformat()
                    }
                    print(f"  ðŸ˜Šâ†’ðŸ˜¢ Emotion changed: {prev_emotion} â†’ {emotion}")
                
                self.current_emotion = emotion
                self.emotion_history.append({
                    "emotion": emotion,
                    "confidence": analysis.get("emotion_confidence", 0.5),
                    "timestamp": datetime.now().isoformat()
                })
                self.last_analysis = analysis
                
                # ðŸ§  ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø§Ø² ØªØµÙˆÛŒØ± (Ø§Ú¯Ù‡ Ú©Ø§Ø±Ø¨Ø± Ù„Ø§Ú¯ÛŒÙ† Ø¨Ø§Ø´Ù‡)
                if self.user_id and self.frame_count % 5 == 0:  # Ù‡Ø± 5 ÙØ±ÛŒÙ…
                    try:
                        img_learn = smart_learner.learn_from_image(self.user_id, frame_base64)
                        if img_learn.get("learned"):
                            self.learned_info.append("image_analysis")
                    except:
                        pass
                
                # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ ØºØ°Ø§
                emotion_info = EMOTION_RESPONSES.get(emotion, EMOTION_RESPONSES["neutral"])
                analysis["food_suggestion"] = emotion_info["food_suggestion"]
                analysis["emotion_fa"] = emotion_info["fa"]
                analysis["emotion_changed"] = emotion_changed
                
                # Ø§Ú¯Ù‡ Ø§Ø­Ø³Ø§Ø³ Ø¹ÙˆØ¶ Ø´Ø¯Ù‡ØŒ Ù¾ÛŒØ§Ù… Ø®Ø§Øµ Ø¨Ø¯Ù‡
                if emotion_changed:
                    if emotion == "sad":
                        analysis["emotion_message"] = "Ú†ÛŒ Ø´Ø¯ØŸ Ù†Ø§Ø±Ø§Ø­Øª Ø´Ø¯ÛŒØŸ ðŸ˜¢"
                    elif emotion == "happy":
                        analysis["emotion_message"] = "Ø®ÙˆØ´Ø­Ø§Ù„ÛŒ Ú©Ù‡ Ø®ÙˆØ´Ø­Ø§Ù„ÛŒ! ðŸ˜Š"
                    elif emotion == "angry":
                        analysis["emotion_message"] = "Ø¢Ø±ÙˆÙ… Ø¨Ø§Ø´ØŒ Ú†ÛŒ Ø´Ø¯Ù‡ØŸ ðŸ¤—"
                
                return analysis
                
            else:
                return {"error": f"API error: {response.status_code}"}
                
        except Exception as e:
            print(f"Frame analysis error: {e}")
            return {"error": str(e), "emotion": "neutral"}
    
    async def chat_with_context(self, message: str, frame_base64: str = None, audio_text: str = None) -> Dict:
        """
        Ú†Øª Ù…Ø¯Ø§ÙˆÙ… Ø¨Ø§ context ÙˆÛŒØ¯ÛŒÙˆÛŒÛŒ Ùˆ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ
        
        Args:
            message: Ù¾ÛŒØ§Ù… Ú©Ø§Ø±Ø¨Ø± (Ù…ØªÙ†ÛŒ ÛŒØ§ Ø§Ø² ØµØ¯Ø§)
            frame_base64: ÙØ±ÛŒÙ… ÙØ¹Ù„ÛŒ (Ø§Ø®ØªÛŒØ§Ø±ÛŒ)
            audio_text: Ù…ØªÙ† ØªØ¨Ø¯ÛŒÙ„ Ø´Ø¯Ù‡ Ø§Ø² ØµØ¯Ø§ (Ø§Ø®ØªÛŒØ§Ø±ÛŒ)
            
        Returns:
            dict Ø¨Ø§ Ù¾Ø§Ø³Ø®ØŒ ØªØ­Ù„ÛŒÙ„ØŒ Ùˆ Ø§Ø·Ù„Ø§Ø¹Ø§Øª ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ
        """
        self.message_count += 1
        
        # Ø§Ú¯Ù‡ Ø§Ø² ØµØ¯Ø§ Ø§ÙˆÙ…Ø¯Ù‡ØŒ Ø§ÙˆÙ† Ø±Ùˆ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†
        user_message = audio_text or message
        
        # Ø§ÙˆÙ„ ÙØ±ÛŒÙ… Ø±Ùˆ ØªØ­Ù„ÛŒÙ„ Ú©Ù† (Ø§Ú¯Ù‡ Ø¯Ø§Ø¯Ù‡ Ø´Ø¯Ù‡)
        if frame_base64:
            analysis = await self.analyze_frame(frame_base64)
        else:
            analysis = self.last_analysis or {"emotion": "neutral"}
        
        emotion = analysis.get("emotion", "neutral")
        emotion_info = EMOTION_RESPONSES.get(emotion, EMOTION_RESPONSES["neutral"])
        
        # ðŸ§  ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ ÙÙˆØ±ÛŒ Ø§Ø² Ù¾ÛŒØ§Ù…
        learned_now = []
        if self.user_id and user_message:
            try:
                learn_result = smart_learner.learn_from_chat(self.user_id, user_message)
                if learn_result.get("learned"):
                    learned_now = learn_result.get("categories", [])
                    self.learned_info.extend(learned_now)
                    print(f"  ðŸ§  Learned from video chat: {learned_now}")
            except Exception as e:
                print(f"  âš ï¸ Learning error: {e}")
        
        # Ø°Ø®ÛŒØ±Ù‡ Ù¾ÛŒØ§Ù… Ú©Ø§Ø±Ø¨Ø± Ø¯Ø± ØªØ§Ø±ÛŒØ®Ú†Ù‡
        self.conversation_history.append({
            "role": "user",
            "content": user_message,
            "emotion": emotion,
            "timestamp": datetime.now().isoformat()
        })
        
        # Ù…Ø­Ø¯ÙˆØ¯ Ú©Ø±Ø¯Ù† ØªØ§Ø±ÛŒØ®Ú†Ù‡
        if len(self.conversation_history) > self.max_history:
            self.conversation_history = self.conversation_history[-self.max_history:]
        
        # Ø³Ø§Ø®Øª context Ø§Ø² ØªØ§Ø±ÛŒØ®Ú†Ù‡
        history_context = "\n".join([
            f"{'Ú©Ø§Ø±Ø¨Ø±' if msg['role'] == 'user' else 'Ø±ÙˆÚ˜Ø§Ù†'}: {msg['content']}"
            for msg in self.conversation_history[-6:]  # Ø¢Ø®Ø±ÛŒÙ† 6 Ù¾ÛŒØ§Ù…
        ])
        
        # Ú¯Ø±ÙØªÙ† Ù¾Ø±ÙˆÙØ§ÛŒÙ„ Ú©Ø§Ø±Ø¨Ø±
        user_profile = ""
        if self.user_id:
            try:
                profile_summary = smart_learner.get_profile_summary(self.user_id)
                warnings = smart_learner.get_warnings(self.user_id)
                if profile_summary:
                    user_profile = f"\nðŸ“‹ Ù¾Ø±ÙˆÙØ§ÛŒÙ„: {profile_summary[:200]}"
                if warnings:
                    user_profile += f"\nâš ï¸ Ù‡Ø´Ø¯Ø§Ø±Ù‡Ø§: {', '.join(warnings[:3])}"
            except:
                pass
        
        # Ø³Ø§Ø®Øª prompt Ú©Ø§Ù…Ù„ Ø¨Ø±Ø§ÛŒ Ù…Ú©Ø§Ù„Ù…Ù‡ Ù…Ø¯Ø§ÙˆÙ…
        full_prompt = f"""ØªÙˆ Ø±ÙˆÚ˜Ø§Ù† Ù‡Ø³ØªÛŒØŒ Ø¯Ø³ØªÛŒØ§Ø± Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø±Ø³ØªÙˆØ±Ø§Ù† Ú©Ù‡ Ø¯Ø§Ø±ÛŒ Ø¨Ø§ Ú©Ø§Ø±Ø¨Ø± ÙˆÛŒØ¯ÛŒÙˆ Ú†Øª Ù…ÛŒâ€ŒÚ©Ù†ÛŒ.

ðŸ“¹ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø§Ø² ØªØµÙˆÛŒØ±:
- Ø§Ø­Ø³Ø§Ø³ Ú©Ø§Ø±Ø¨Ø±: {emotion} ({emotion_info["fa"]})
- Ù…Ø­ÛŒØ·: {analysis.get("environment", "Ù†Ø§Ù…Ø´Ø®Øµ")}
- Ù„Ø­Ù† Ù¾Ø§Ø³Ø®: {emotion_info["response_tone"]}
{user_profile}

ðŸ’¬ ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ù…Ú©Ø§Ù„Ù…Ù‡:
{history_context}

Ù‚ÙˆØ§Ù†ÛŒÙ† Ù…Ù‡Ù…:
1. Ø§ÛŒÙ† ÛŒÚ© Ù…Ú©Ø§Ù„Ù…Ù‡ Ù…Ø¯Ø§ÙˆÙ… Ø§Ø³Øª - Ø¨Ù‡ ØªØ§Ø±ÛŒØ®Ú†Ù‡ ØªÙˆØ¬Ù‡ Ú©Ù†
2. Ù…ØªÙ†Ø§Ø³Ø¨ Ø¨Ø§ Ø§Ø­Ø³Ø§Ø³ Ú©Ø§Ø±Ø¨Ø± ({emotion_info["fa"]}) Ø¬ÙˆØ§Ø¨ Ø¨Ø¯Ù‡
3. ÙØ§Ø±Ø³ÛŒ Ù…Ø­Ø§ÙˆØ±Ù‡â€ŒØ§ÛŒ Ùˆ ØµÙ…ÛŒÙ…ÛŒ ØµØ­Ø¨Øª Ú©Ù†
4. Ø¬ÙˆØ§Ø¨â€ŒÙ‡Ø§ Ú©ÙˆØªØ§Ù‡ Ø¨Ø§Ø´Ù† (Ø¨Ø±Ø§ÛŒ Ù…Ú©Ø§Ù„Ù…Ù‡ ØµÙˆØªÛŒ)
5. Ø§Ú¯Ù‡ Ú†ÛŒØ² Ø¬Ø¯ÛŒØ¯ÛŒ ÛŒØ§Ø¯ Ú¯Ø±ÙØªÛŒØŒ ØªØ£ÛŒÛŒØ¯ Ú©Ù†
6. Ø§Ú¯Ù‡ ØºØ°Ø§ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Ù…ÛŒØ¯ÛŒØŒ Ø¯Ù„ÛŒÙ„Ø´ Ø±Ùˆ Ø¨Ú¯Ùˆ

{"ðŸ§  Ø§Ù„Ø§Ù† ÛŒØ§Ø¯ Ú¯Ø±ÙØªÙ…: " + ", ".join(learned_now) if learned_now else ""}

Ù¾ÛŒØ§Ù… Ø¬Ø¯ÛŒØ¯ Ú©Ø§Ø±Ø¨Ø±: {user_message}
"""
        
        try:
            url = f"{self.api_url}/{VISION_MODEL}:generateContent"
            headers = {
                "Authorization": f"Bearer {GAPGPT_API_KEY}",
                "Content-Type": "application/json"
            }
            
            parts = [{"text": full_prompt}]
            
            # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† ØªØµÙˆÛŒØ± Ø§Ú¯Ù‡ Ø¯Ø§Ø±ÛŒÙ…
            if frame_base64:
                img_data = frame_base64.split(",")[1] if "," in frame_base64 else frame_base64
                parts.append({
                    "inline_data": {
                        "mime_type": "image/jpeg",
                        "data": img_data
                    }
                })
            
            data = {
                "contents": [{"parts": parts}],
                "generationConfig": {
                    "temperature": 0.7,
                    "maxOutputTokens": 300  # Ú©ÙˆØªØ§Ù‡â€ŒØªØ± Ø¨Ø±Ø§ÛŒ Ù…Ú©Ø§Ù„Ù…Ù‡ ØµÙˆØªÛŒ
                }
            }
            
            response = requests.post(url, headers=headers, json=data, timeout=30)
            
            if response.ok:
                result = response.json()
                ai_response = result['candidates'][0]['content']['parts'][0]['text'].strip()
                
                # Ø°Ø®ÛŒØ±Ù‡ Ù¾Ø§Ø³Ø® Ø¯Ø± ØªØ§Ø±ÛŒØ®Ú†Ù‡
                self.conversation_history.append({
                    "role": "assistant",
                    "content": ai_response,
                    "timestamp": datetime.now().isoformat()
                })
                
                return {
                    "response": ai_response,
                    "emotion": emotion,
                    "emotion_fa": emotion_info["fa"],
                    "tone": emotion_info["response_tone"],
                    "analysis": analysis,
                    "learned": learned_now,
                    "message_count": self.message_count,
                    "emotion_changed": analysis.get("emotion_changed", False)
                }
            else:
                return {"error": f"API error: {response.status_code}", "response": "Ù…ØªØ£Ø³ÙÙ…ØŒ Ù…Ø´Ú©Ù„ÛŒ Ù¾ÛŒØ´ Ø§ÙˆÙ…Ø¯."}
                
        except Exception as e:
            print(f"Chat error: {e}")
            return {"error": str(e), "response": "Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´"}
    
    async def transcribe_and_chat(self, audio_base64: str, frame_base64: str = None, mime_type: str = "audio/webm") -> Dict:
        """
        ðŸŽ¤ Ø¯Ø±ÛŒØ§ÙØª ØµØ¯Ø§ØŒ ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ Ù…ØªÙ†ØŒ Ùˆ Ú†Øª
        
        Args:
            audio_base64: ØµØ¯Ø§ÛŒ Ú©Ø§Ø±Ø¨Ø±
            frame_base64: ÙØ±ÛŒÙ… ÙØ¹Ù„ÛŒ
            mime_type: Ù†ÙˆØ¹ ÙØ§ÛŒÙ„ ØµÙˆØªÛŒ
            
        Returns:
            dict Ø¨Ø§ Ù…ØªÙ†ØŒ Ù¾Ø§Ø³Ø®ØŒ Ùˆ ØªØ­Ù„ÛŒÙ„
        """
        from gemini_client import transcribe
        
        try:
            # ØªØ¨Ø¯ÛŒÙ„ ØµØ¯Ø§ Ø¨Ù‡ Ù…ØªÙ† Ø¨Ø§ ØªØ´Ø®ÛŒØµ Ø§Ø­Ø³Ø§Ø³
            transcribe_result = transcribe(audio_base64, mime_type, detect_emotion=True)
            
            if isinstance(transcribe_result, dict):
                user_text = transcribe_result.get("text", "")
                audio_emotion = transcribe_result.get("emotion", "neutral")
            else:
                user_text = transcribe_result
                audio_emotion = "neutral"
            
            if not user_text:
                return {"error": "Ù…ØªÙ†ÛŒ ØªØ´Ø®ÛŒØµ Ø¯Ø§Ø¯Ù‡ Ù†Ø´Ø¯", "response": "Ù…ØªÙˆØ¬Ù‡ Ù†Ø´Ø¯Ù…ØŒ Ù…ÛŒØ´Ù‡ ØªÚ©Ø±Ø§Ø± Ú©Ù†ÛŒØŸ"}
            
            # Ú†Øª Ø¨Ø§ Ù…ØªÙ† Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯Ù‡
            chat_result = await self.chat_with_context(user_text, frame_base64, audio_text=user_text)
            
            # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø§Ø·Ù„Ø§Ø¹Ø§Øª ØµÙˆØªÛŒ
            chat_result["transcribed_text"] = user_text
            chat_result["audio_emotion"] = audio_emotion
            
            return chat_result
            
        except Exception as e:
            print(f"Transcribe and chat error: {e}")
            return {"error": str(e), "response": "Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ ØµØ¯Ø§"}


# Ø°Ø®ÛŒØ±Ù‡ sessions ÙØ¹Ø§Ù„
video_sessions: Dict[str, VideoChatSession] = {}


async def create_video_session(user_id: int) -> VideoChatSession:
    """Ø³Ø§Ø®Øª session Ø¬Ø¯ÛŒØ¯"""
    session = VideoChatSession(user_id)
    await session.start()
    video_sessions[session.session_id] = session
    return session


async def get_video_session(session_id: str) -> Optional[VideoChatSession]:
    """Ú¯Ø±ÙØªÙ† session Ù…ÙˆØ¬ÙˆØ¯"""
    return video_sessions.get(session_id)


async def close_video_session(session_id: str) -> Dict:
    """Ø¨Ø³ØªÙ† session"""
    session = video_sessions.pop(session_id, None)
    if session:
        return await session.stop()
    return {"error": "Session not found"}
